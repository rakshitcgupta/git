{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents the main part of the project. It is decomposed in the following parts:\n",
    "- Parameters setting \n",
    "- Creation of the trading environment \n",
    "- Set-up of the trading agent (actor)\n",
    "- Set-up of the portfolio vector memory (PVM)\n",
    "- Agent training \n",
    "- Agent Evaluation\n",
    "- Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Note:</u> This notebook has been cleaned up and run on a local machine. The appearing results are only for illustration and not representative of the project results in the presentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BRBAcvyDeOBF"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T00:38:45.305452Z",
     "start_time": "2018-07-08T00:38:41.954075Z"
    },
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1cA1tOpgeOBG",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ffn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1dbd5f6f25a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mffn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0menvironment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ffn'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import pandas as pd\n",
    "import ffn\n",
    "from environment import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xU0NVhPpd4y6"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T00:38:45.721074Z",
     "start_time": "2018-07-08T00:38:45.307419Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset\n",
    "\n",
    "#can be changed following the type of stocks studied \n",
    "\n",
    "path_data = './np_data/inputCrypto.npy'\n",
    "\n",
    "\n",
    "data_type = path_data.split('/')[2][5:].split('.')[0]\n",
    "namesBio=['JNJ','PFE','AMGN','MDT','CELG','LLY']\n",
    "namesUtilities=['XOM','CVX','MRK','SLB','MMM']\n",
    "namesTech=['FB','AMZN','MSFT','AAPL','T','VZ','CMCSA','IBM','CRM','INTC']\n",
    "namesCrypto = ['ETCBTC', 'ETHBTC', 'DOGEBTC', 'ETHUSDT', 'BTCUSDT', 'XRPBTC', 'DASHBTC', 'XMRBTC', 'LTCBTC', 'ETCETH']\n",
    "\n",
    "\n",
    "if data_type == 'Utilities':\n",
    "    list_stock = namesUtilities\n",
    "elif data_type == 'Bio':\n",
    "    list_stock = namesBio\n",
    "elif data_type == 'Tech':\n",
    "    list_stock = namesTech\n",
    "elif data_type == 'Crypto':\n",
    "    list_stock = namesCrypto\n",
    "else:\n",
    "    list_stock = [i for i in range(m)]\n",
    "\n",
    "\n",
    "# determine the length of the data, #features, #stocks\n",
    "data = np.load(path_data)\n",
    "trading_period = data.shape[2]\n",
    "nb_feature_map = data.shape[0]\n",
    "nb_stocks = data.shape[1]\n",
    "\n",
    "# fix parameters of the network\n",
    "m = nb_stocks\n",
    "\n",
    "###############################dictionaries of the problem###########################\n",
    "dict_hp_net = {'n_filter_1': 2, 'n_filter_2': 20, 'kernel1_size':(1, 3)}\n",
    "dict_hp_pb = {'batch_size': 50, 'ratio_train': 0.6,'ratio_val': 0.2, 'length_tensor': 10,\n",
    "              'ratio_greedy':0.8, 'ratio_regul': 0.1}\n",
    "dict_hp_opt = {'regularization': 1e-8, 'learning': 9e-2}\n",
    "dict_fin = {'trading_cost': 0.25/100, 'interest_rate': 0.02/250, 'cash_bias_init': 0.7}\n",
    "dict_train = {'pf_init_train': 10000, 'w_init_train': 'd', 'n_episodes':2, 'n_batches':10}\n",
    "dict_test = {'pf_init_test': 10000, 'w_init_test': 'd'}\n",
    "\n",
    "\n",
    "###############################HP of the network ###########################\n",
    "n_filter_1 = dict_hp_net['n_filter_1']\n",
    "n_filter_2 = dict_hp_net['n_filter_2']\n",
    "kernel1_size = dict_hp_net['kernel1_size']\n",
    "\n",
    "###############################HP of the problem###########################\n",
    "\n",
    "# Size of mini-batch during training\n",
    "batch_size = dict_hp_pb['batch_size']\n",
    "# Total number of steps for pre-training in the training set\n",
    "total_steps_train = int(dict_hp_pb['ratio_train']*trading_period)\n",
    "\n",
    "# Total number of steps for pre-training in the validation set\n",
    "total_steps_val = int(dict_hp_pb['ratio_val']*trading_period)\n",
    "\n",
    "# Total number of steps for the test\n",
    "total_steps_test = trading_period-total_steps_train-total_steps_val\n",
    "\n",
    "# Number of the columns (number of the trading periods) in each input price matrix\n",
    "n = dict_hp_pb['length_tensor']\n",
    "\n",
    "ratio_greedy = dict_hp_pb['ratio_greedy']\n",
    "\n",
    "ratio_regul = dict_hp_pb['ratio_regul']\n",
    "\n",
    "##############################HP of the optimization###########################\n",
    "\n",
    "\n",
    "# The L2 regularization coefficient applied to network training\n",
    "regularization = dict_hp_opt['regularization']\n",
    "# Parameter alpha (i.e. the step size) of the Adam optimization\n",
    "learning = dict_hp_opt['learning']\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning)\n",
    "\n",
    "\n",
    "##############################Finance parameters###########################\n",
    "\n",
    "trading_cost= dict_fin['trading_cost']\n",
    "interest_rate= dict_fin['interest_rate']\n",
    "cash_bias_init = dict_fin['cash_bias_init']\n",
    "\n",
    "############################## PVM Parameters ###########################\n",
    "sample_bias = 5e-5  # Beta in the geometric distribution for online training sample batches\n",
    "\n",
    "\n",
    "############################## Training Parameters ###########################\n",
    "\n",
    "w_init_train = np.array(np.array([1]+[0]*m))#dict_train['w_init_train']\n",
    "\n",
    "pf_init_train = dict_train['pf_init_train']\n",
    "\n",
    "n_episodes = dict_train['n_episodes']\n",
    "n_batches = dict_train['n_batches']\n",
    "\n",
    "############################## Test Parameters ###########################\n",
    "\n",
    "w_init_test = np.array(np.array([1]+[0]*m))#dict_test['w_init_test']\n",
    "\n",
    "pf_init_test = dict_test['pf_init_test']\n",
    "\n",
    "\n",
    "############################## other environment Parameters ###########################\n",
    "\n",
    "w_eq = np.array(np.array([1/(m+1)]*(m+1)))\n",
    "\n",
    "w_s = np.array(np.array([1]+[0.0]*m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T00:38:45.733332Z",
     "start_time": "2018-07-08T00:38:45.723789Z"
    }
   },
   "outputs": [],
   "source": [
    "#random action function\n",
    "\n",
    "def get_random_action(m):\n",
    "    random_vec = np.random.rand(m+1)\n",
    "    return random_vec/np.sum(random_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T00:38:45.738422Z",
     "start_time": "2018-07-08T00:38:45.736043Z"
    }
   },
   "outputs": [],
   "source": [
    "#get_random_action(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T00:38:45.794570Z",
     "start_time": "2018-07-08T00:38:45.740993Z"
    },
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4UaHchSfeOBN"
   },
   "outputs": [],
   "source": [
    "#environment for trading of the agent \n",
    "# this is the agent trading environment (policy network agent)\n",
    "env = TradeEnv(path=path_data, window_length=n,\n",
    "               portfolio_value=pf_init_train, trading_cost=trading_cost,\n",
    "               interest_rate=interest_rate, train_size=dict_hp_pb['ratio_train'])\n",
    "\n",
    "\n",
    "#environment for equiweighted\n",
    "#this environment is set up for an agent who only plays an equiweithed portfolio (baseline)\n",
    "env_eq = TradeEnv(path=path_data, window_length=n,\n",
    "               portfolio_value=pf_init_train, trading_cost=trading_cost,\n",
    "               interest_rate=interest_rate, train_size=dict_hp_pb['ratio_train'])\n",
    "\n",
    "#environment secured (only money)\n",
    "#this environment is set up for an agentwho plays secure, keeps its money\n",
    "env_s = TradeEnv(path=path_data, window_length=n,\n",
    "               portfolio_value=pf_init_train, trading_cost=trading_cost,\n",
    "               interest_rate=interest_rate, train_size=dict_hp_pb['ratio_train'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T00:38:45.936744Z",
     "start_time": "2018-07-08T00:38:45.797252Z"
    }
   },
   "outputs": [],
   "source": [
    "#full on one stock environment \n",
    "#these environments are set up for agents who play only on one stock\n",
    "\n",
    "action_fu = list()\n",
    "env_fu = list()\n",
    "\n",
    "\n",
    "for i in range(m):\n",
    "    action = np.array([0]*(i+1) + [1] + [0]*(m-(i+1)))\n",
    "    action_fu.append(action)\n",
    "    \n",
    "    env_fu_i = TradeEnv(path=path_data, window_length=n,\n",
    "               portfolio_value=pf_init_train, trading_cost=trading_cost,\n",
    "               interest_rate=interest_rate, train_size=dict_hp_pb['ratio_train'])\n",
    "    \n",
    "    env_fu.append(env_fu_i)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T00:38:46.706796Z",
     "start_time": "2018-07-08T00:38:45.939668Z"
    },
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "aMEr2sNKeOBU"
   },
   "outputs": [],
   "source": [
    "# define neural net \\pi_\\phi(s) as a class\n",
    "class Policy(object):\n",
    "    '''\n",
    "    This class is used to instanciate the policy network agent\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, m, n, sess, optimizer,\n",
    "                 trading_cost=trading_cost,\n",
    "                 interest_rate=interest_rate,\n",
    "                 n_filter_1=n_filter_1,\n",
    "                 n_filter_2=n_filter_2):\n",
    "\n",
    "        # parameters\n",
    "        self.trading_cost = trading_cost\n",
    "        self.interest_rate = interest_rate\n",
    "        self.n_filter_1 = n_filter_1\n",
    "        self.n_filter_2 = n_filter_2\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "\n",
    "        with tf.variable_scope(\"Inputs\"):\n",
    "\n",
    "            # Placeholder\n",
    "\n",
    "            # tensor of the prices\n",
    "            self.X_t = tf.placeholder(\n",
    "                tf.float32, [None, nb_feature_map, self.m, self.n])  # The Price tensor\n",
    "            # weights at the previous time step\n",
    "            self.W_previous = tf.placeholder(tf.float32, [None, self.m+1])\n",
    "            # portfolio value at the previous time step\n",
    "            self.pf_value_previous = tf.placeholder(tf.float32, [None, 1])\n",
    "            # vector of Open(t+1)/Open(t)\n",
    "            self.dailyReturn_t = tf.placeholder(tf.float32, [None, self.m])\n",
    "            \n",
    "            #self.pf_value_previous_eq = tf.placeholder(tf.float32, [None, 1])\n",
    "            \n",
    "            \n",
    "\n",
    "        with tf.variable_scope(\"Policy_Model\"):\n",
    "\n",
    "            # variable of the cash bias\n",
    "            bias = tf.get_variable('cash_bias', shape=[\n",
    "                                   1, 1, 1, 1], initializer=tf.constant_initializer(cash_bias_init))\n",
    "            # shape of the tensor == batchsize\n",
    "            shape_X_t = tf.shape(self.X_t)[0]\n",
    "            # trick to get a \"tensor size\" for the cash bias\n",
    "            self.cash_bias = tf.tile(bias, tf.stack([shape_X_t, 1, 1, 1]))\n",
    "            # print(self.cash_bias.shape)\n",
    "\n",
    "            with tf.variable_scope(\"Conv1\"):\n",
    "                # first layer on the X_t tensor\n",
    "                # return a tensor of depth 2\n",
    "                self.conv1 = tf.layers.conv2d(\n",
    "                    inputs=tf.transpose(self.X_t, perm=[0, 3, 2, 1]),\n",
    "                    activation=tf.nn.relu,\n",
    "                    filters=self.n_filter_1,\n",
    "                    strides=(1, 1),\n",
    "                    kernel_size=kernel1_size,\n",
    "                    padding='same')\n",
    "\n",
    "            with tf.variable_scope(\"Conv2\"):\n",
    "                \n",
    "                #feature maps\n",
    "                self.conv2 = tf.layers.conv2d(\n",
    "                    inputs=self.conv1,\n",
    "                    activation=tf.nn.relu,\n",
    "                    filters=self.n_filter_2,\n",
    "                    strides=(self.n, 1),\n",
    "                    kernel_size=(1, self.n),\n",
    "                    padding='same')\n",
    "\n",
    "            with tf.variable_scope(\"Tensor3\"):\n",
    "                #w from last periods\n",
    "                # trick to have good dimensions\n",
    "                w_wo_c = self.W_previous[:, 1:]\n",
    "                w_wo_c = tf.expand_dims(w_wo_c, 1)\n",
    "                w_wo_c = tf.expand_dims(w_wo_c, -1)\n",
    "                self.tensor3 = tf.concat([self.conv2, w_wo_c], axis=3)\n",
    "\n",
    "            with tf.variable_scope(\"Conv3\"):\n",
    "                #last feature map WITHOUT cash bias\n",
    "                self.conv3 = tf.layers.conv2d(\n",
    "                    inputs=self.conv2,\n",
    "                    activation=tf.nn.relu,\n",
    "                    filters=1,\n",
    "                    strides=(self.n_filter_2 + 1, 1),\n",
    "                    kernel_size=(1, 1),\n",
    "                    padding='same')\n",
    "\n",
    "            with tf.variable_scope(\"Tensor4\"):\n",
    "                #last feature map WITH cash bias\n",
    "                self.tensor4 = tf.concat([self.cash_bias, self.conv3], axis=2)\n",
    "                # we squeeze to reduce and get the good dimension\n",
    "                self.squeezed_tensor4 = tf.squeeze(self.tensor4, [1, 3])\n",
    "\n",
    "            with tf.variable_scope(\"Policy_Output\"):\n",
    "                # softmax layer to obtain weights\n",
    "                self.action = tf.nn.softmax(self.squeezed_tensor4)\n",
    "\n",
    "            with tf.variable_scope(\"Reward\"):\n",
    "                # computation of the reward\n",
    "                #please look at the chronological map to understand\n",
    "                constant_return = tf.constant(\n",
    "                    1+self.interest_rate, shape=[1, 1])\n",
    "                cash_return = tf.tile(\n",
    "                    constant_return, tf.stack([shape_X_t, 1]))\n",
    "                y_t = tf.concat(\n",
    "                    [cash_return, self.dailyReturn_t], axis=1)\n",
    "                Vprime_t = self.action * self.pf_value_previous\n",
    "                Vprevious = self.W_previous*self.pf_value_previous\n",
    "\n",
    "                # this is just a trick to get the good shape for cost\n",
    "                constant = tf.constant(1.0, shape=[1])\n",
    "\n",
    "                cost = self.trading_cost * \\\n",
    "                    tf.norm(Vprime_t-Vprevious, ord=1, axis=1)*constant\n",
    "\n",
    "                cost = tf.expand_dims(cost, 1)\n",
    "\n",
    "                zero = tf.constant(\n",
    "                    np.array([0.0]*m).reshape(1, m), shape=[1, m], dtype=tf.float32)\n",
    "\n",
    "                vec_zero = tf.tile(zero, tf.stack([shape_X_t, 1]))\n",
    "                vec_cost = tf.concat([cost, vec_zero], axis=1)\n",
    "\n",
    "                Vsecond_t = Vprime_t - vec_cost\n",
    "\n",
    "                V_t = tf.multiply(Vsecond_t, y_t)\n",
    "                self.portfolioValue = tf.norm(V_t, ord=1)\n",
    "                self.instantaneous_reward = (\n",
    "                    self.portfolioValue-self.pf_value_previous)/self.pf_value_previous\n",
    "                \n",
    "                \n",
    "            with tf.variable_scope(\"Reward_Equiweighted\"):\n",
    "                constant_return = tf.constant(\n",
    "                    1+self.interest_rate, shape=[1, 1])\n",
    "                cash_return = tf.tile(\n",
    "                    constant_return, tf.stack([shape_X_t, 1]))\n",
    "                y_t = tf.concat(\n",
    "                    [cash_return, self.dailyReturn_t], axis=1)\n",
    "  \n",
    "\n",
    "                V_eq = w_eq*self.pf_value_previous\n",
    "                V_eq_second = tf.multiply(V_eq, y_t)\n",
    "        \n",
    "                self.portfolioValue_eq = tf.norm(V_eq_second, ord=1)\n",
    "            \n",
    "                self.instantaneous_reward_eq = (\n",
    "                    self.portfolioValue_eq-self.pf_value_previous)/self.pf_value_previous\n",
    "                \n",
    "            with tf.variable_scope(\"Max_weight\"):\n",
    "                self.max_weight = tf.reduce_max(self.action)\n",
    "                print(self.max_weight.shape)\n",
    "\n",
    "                \n",
    "            with tf.variable_scope(\"Reward_adjusted\"):\n",
    "                \n",
    "                self.adjested_reward = self.instantaneous_reward - self.instantaneous_reward_eq - ratio_regul*self.max_weight\n",
    "                \n",
    "        #objective function \n",
    "        #maximize reward over the batch \n",
    "        # min(-r) = max(r)\n",
    "        self.train_op = optimizer.minimize(-self.adjested_reward)\n",
    "        \n",
    "        # some bookkeeping\n",
    "        self.optimizer = optimizer\n",
    "        self.sess = sess\n",
    "\n",
    "    def compute_W(self, X_t_, W_previous_):\n",
    "        \"\"\"\n",
    "        This function returns the action the agent takes \n",
    "        given the input tensor and the W_previous\n",
    "        \n",
    "        It is a vector of weight\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return self.sess.run(tf.squeeze(self.action), feed_dict={self.X_t: X_t_, self.W_previous: W_previous_})\n",
    "\n",
    "    def train(self, X_t_, W_previous_, pf_value_previous_, dailyReturn_t_):\n",
    "        \"\"\"\n",
    "        This function trains the neural network\n",
    "        maximizing the reward \n",
    "        the input is a batch of the differents values\n",
    "        \"\"\"\n",
    "        self.sess.run(self.train_op, feed_dict={self.X_t: X_t_,\n",
    "                                                self.W_previous: W_previous_,\n",
    "                                                self.pf_value_previous: pf_value_previous_,\n",
    "                                                self.dailyReturn_t: dailyReturn_t_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the PVM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T00:38:46.746472Z",
     "start_time": "2018-07-08T00:38:46.709092Z"
    },
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "p959bcH_AJdJ"
   },
   "outputs": [],
   "source": [
    "class PVM(object):\n",
    "    '''\n",
    "    This is the memory stack called PVM in the paper\n",
    "    '''\n",
    "\n",
    "    def __init__(self, m, sample_bias, total_steps = total_steps_train, \n",
    "                 batch_size = batch_size, w_init = w_init_train):\n",
    "        \n",
    "        \n",
    "        #initialization of the memory \n",
    "        #we have a total_step_times the initialization portfolio tensor \n",
    "        self.memory = np.transpose(np.array([w_init]*total_steps))  \n",
    "        self.sample_bias = sample_bias\n",
    "        self.total_steps = total_steps\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get_W(self, t):\n",
    "        #return the weight from the PVM at time t \n",
    "        return self.memory[:, t]\n",
    "\n",
    "    def update(self, t, w):\n",
    "        #update the weight at time t\n",
    "        self.memory[:, t] = w\n",
    "\n",
    "\n",
    "    def draw(self, beta=sample_bias):\n",
    "        '''\n",
    "        returns a valid step so you can get a training batch starting at this step\n",
    "        '''\n",
    "        while 1:\n",
    "            z = np.random.geometric(p=beta)\n",
    "            tb = self.total_steps - self.batch_size + 1 - z\n",
    "            if tb >= 0:\n",
    "                return tb\n",
    "            \n",
    "    def test(self):\n",
    "        #just to test\n",
    "        return self.memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AUT4gLhveOBW"
   },
   "source": [
    "Try to rollout trajecories using the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T00:38:46.753814Z",
     "start_time": "2018-07-08T00:38:46.748415Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_max_draw_down(xs):\n",
    "    xs = np.array(xs)\n",
    "    i = np.argmax(np.maximum.accumulate(xs) - xs) # end of the period\n",
    "    j = np.argmax(xs[:i]) # start of period\n",
    "    \n",
    "    return xs[j] - xs[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T00:38:46.960918Z",
     "start_time": "2018-07-08T00:38:46.756178Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_perf(e):\n",
    "    \"\"\"\n",
    "    This function evaluates the performance of the different types of agents. \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    list_weight_end_val = list()\n",
    "    list_pf_end_training = list()\n",
    "    list_pf_min_training = list()\n",
    "    list_pf_max_training = list()\n",
    "    list_pf_mean_training = list()\n",
    "    list_pf_dd_training = list()\n",
    "    \n",
    "    #######TEST#######\n",
    "    #environment for trading of the agent \n",
    "    env_eval = TradeEnv(path=path_data, window_length=n,\n",
    "                   portfolio_value=pf_init_train, trading_cost=trading_cost,\n",
    "                   interest_rate=interest_rate, train_size=dict_hp_pb['ratio_train'])\n",
    "\n",
    "\n",
    "\n",
    "    #initialization of the environment \n",
    "    state_eval, done_eval = env_eval.reset(w_init_test, pf_init_test, t = total_steps_train)\n",
    "\n",
    "\n",
    "\n",
    "    #first element of the weight and portfolio value \n",
    "    p_list_eval = [pf_init_test]\n",
    "    w_list_eval = [w_init_test]\n",
    "\n",
    "    for k in range(total_steps_train, total_steps_train +total_steps_val-int(n/2)):\n",
    "        X_t = state_eval[0].reshape([-1]+ list(state_eval[0].shape))\n",
    "        W_previous = state_eval[1].reshape([-1]+ list(state_eval[1].shape))\n",
    "        pf_value_previous = state_eval[2]\n",
    "        #compute the action \n",
    "        action = actor.compute_W(X_t, W_previous)\n",
    "        #step forward environment \n",
    "        state_eval, reward_eval, done_eval = env_eval.step(action)\n",
    "\n",
    "        X_next = state_eval[0]\n",
    "        W_t_eval = state_eval[1]\n",
    "        pf_value_t_eval = state_eval[2]\n",
    "\n",
    "        dailyReturn_t = X_next[-1, :, -1]\n",
    "        #print('current portfolio value', round(pf_value_previous,0))\n",
    "        #print('weights', W_previous)\n",
    "        p_list_eval.append(pf_value_t_eval)\n",
    "        w_list_eval.append(W_t_eval)\n",
    "        \n",
    "    list_weight_end_val.append(w_list_eval[-1])\n",
    "    list_pf_end_training.append(p_list_eval[-1])\n",
    "    list_pf_min_training.append(np.min(p_list_eval))\n",
    "    list_pf_max_training.append(np.max(p_list_eval))\n",
    "    list_pf_mean_training.append(np.mean(p_list_eval))\n",
    "    \n",
    "    list_pf_dd_training.append(get_max_draw_down(p_list_eval))\n",
    "\n",
    "    print('End of test PF value:',round(p_list_eval[-1]))\n",
    "    print('Min of test PF value:',round(np.min(p_list_eval)))\n",
    "    print('Max of test PF value:',round(np.max(p_list_eval)))\n",
    "    print('Mean of test PF value:',round(np.mean(p_list_eval)))\n",
    "    print('Max Draw Down of test PF value:',round(get_max_draw_down(p_list_eval)))\n",
    "    print('End of test weights:',w_list_eval[-1])\n",
    "    plt.title('Portfolio evolution (validation set) episode {}'.format(e))\n",
    "    plt.plot(p_list_eval, label = 'Agent Portfolio Value')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "    plt.title('Portfolio weights (end of validation set) episode {}'.format(e))\n",
    "    plt.bar(np.arange(m+1), list_weight_end_val[-1])\n",
    "    plt.xticks(np.arange(m+1), ['Money'] + list_stock, rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    names = ['Money'] + list_stock\n",
    "    w_list_eval = np.array(w_list_eval)\n",
    "    for j in range(m+1):\n",
    "        plt.plot(w_list_eval[:,j], label = 'Weight Stock {}'.format(names[j]))\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T02:30:27.617009Z",
     "start_time": "2018-07-08T00:38:46.963263Z"
    },
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hNxzXPFleOBi",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############# TRAINING #####################\n",
    "###########################################\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# sess\n",
    "sess = tf.Session()\n",
    "\n",
    "# initialize networks\n",
    "actor = Policy(m, n, sess, optimizer,\n",
    "                 trading_cost=trading_cost, \n",
    "                 interest_rate=interest_rate)  # policy initialization\n",
    "\n",
    "# initialize tensorflow graphs\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "\n",
    "list_final_pf = list()\n",
    "list_final_pf_eq = list()\n",
    "list_final_pf_s = list()\n",
    "\n",
    "list_final_pf_fu = list()\n",
    "state_fu = [0]*m\n",
    "done_fu = [0]*m\n",
    "\n",
    "pf_value_t_fu = [0]*m\n",
    "\n",
    "for i in range(m):\n",
    "    list_final_pf_fu.append(list())\n",
    "    \n",
    "\n",
    "###### Train #####\n",
    "for e in range(n_episodes):\n",
    "    print('Start Episode', e)\n",
    "    if e==0:\n",
    "        eval_perf('Before Training')\n",
    "    print('Episode:', e)\n",
    "    #init the PVM with the training parameters\n",
    "    memory = PVM(m,sample_bias, total_steps = total_steps_train, \n",
    "                 batch_size = batch_size, w_init = w_init_train)\n",
    "    \n",
    "    for nb in range(n_batches):\n",
    "        #draw the starting point of the batch \n",
    "        i_start = memory.draw()\n",
    "        \n",
    "        \n",
    "        #reset the environment with the weight from PVM at the starting point\n",
    "        #reset also with a portfolio value with initial portfolio value\n",
    "        state, done = env.reset(memory.get_W(i_start), pf_init_train, t=i_start )\n",
    "        state_eq, done_eq = env_eq.reset(w_eq, pf_init_train, t=i_start )\n",
    "        state_s, done_s = env_s.reset(w_s, pf_init_train, t=i_start )\n",
    "        \n",
    "        for i in range(m):\n",
    "            state_fu[i], done_fu[i] = env_fu[i].reset(action_fu[i], pf_init_train, t=i_start )\n",
    "        \n",
    "        \n",
    "        \n",
    "        list_X_t, list_W_previous, list_pf_value_previous, list_dailyReturn_t = [], [], [], []\n",
    "        list_pf_value_previous_eq, list_pf_value_previous_s = [],[]\n",
    "        list_pf_value_previous_fu = list()\n",
    "        for i in range(m):\n",
    "            list_pf_value_previous_fu.append(list())\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for bs in range(batch_size):\n",
    "            \n",
    "            #load the different inputs from the previous loaded state \n",
    "            X_t = state[0].reshape([-1] + list(state[0].shape))\n",
    "            W_previous = state[1].reshape([-1] + list(state[1].shape))\n",
    "            pf_value_previous = state[2]\n",
    "            \n",
    "            \n",
    "            if np.random.rand()< ratio_greedy:\n",
    "                #print('go')\n",
    "                #computation of the action of the agent\n",
    "                action = actor.compute_W(X_t, W_previous)\n",
    "            else:\n",
    "                action = get_random_action(m)\n",
    "            \n",
    "            #given the state and the action, call the environment to go one time step later \n",
    "            state, reward, done = env.step(action)\n",
    "            state_eq, reward_eq, done_eq = env_eq.step(w_eq)\n",
    "            state_s, reward_s, done_s = env_s.step(w_s)\n",
    "            \n",
    "            for i in range(m):\n",
    "                state_fu[i], _ , done_fu[i] = env_fu[i].step(action_fu[i])\n",
    "\n",
    "            \n",
    "            \n",
    "            #get the new state \n",
    "            X_next = state[0]\n",
    "            W_t = state[1]\n",
    "            pf_value_t = state[2]\n",
    "            \n",
    "            pf_value_t_eq = state_eq[2]\n",
    "            pf_value_t_s = state_s[2]\n",
    "            \n",
    "            for i in range(m):\n",
    "                pf_value_t_fu[i] = state_fu[i][2]\n",
    "                \n",
    "            \n",
    "            #let us compute the returns \n",
    "            dailyReturn_t = X_next[-1, :, -1]\n",
    "            #update into the PVM\n",
    "            memory.update(i_start+bs, W_t)\n",
    "            #store elements\n",
    "            list_X_t.append(X_t.reshape(state[0].shape))\n",
    "            list_W_previous.append(W_previous.reshape(state[1].shape))\n",
    "            list_pf_value_previous.append([pf_value_previous])\n",
    "            list_dailyReturn_t.append(dailyReturn_t)\n",
    "            \n",
    "            list_pf_value_previous_eq.append(pf_value_t_eq)\n",
    "            list_pf_value_previous_s.append(pf_value_t_s)\n",
    "            \n",
    "            for i in range(m):\n",
    "                list_pf_value_previous_fu[i].append(pf_value_t_fu[i])\n",
    "            \n",
    "            if bs==batch_size-1:\n",
    "                list_final_pf.append(pf_value_t)\n",
    "                list_final_pf_eq.append(pf_value_t_eq)\n",
    "                list_final_pf_s.append(pf_value_t_s)\n",
    "                for i in range(m):\n",
    "                    list_final_pf_fu[i].append(pf_value_t_fu[i])\n",
    "\n",
    "            \n",
    "            \n",
    "#             #printing\n",
    "#             if bs==0:\n",
    "#                 print('start', i_start)\n",
    "#                 print('PF_start', round(pf_value_previous,0))\n",
    "\n",
    "#             if bs==batch_size-1:\n",
    "#                 print('PF_end', round(pf_value_t,0))\n",
    "#                 print('weight', W_t)\n",
    "\n",
    "        list_X_t = np.array(list_X_t)\n",
    "        list_W_previous = np.array(list_W_previous)\n",
    "        list_pf_value_previous = np.array(list_pf_value_previous)\n",
    "        list_dailyReturn_t = np.array(list_dailyReturn_t)\n",
    "        \n",
    "        \n",
    "        #for each batch, train the network to maximize the reward\n",
    "        actor.train(list_X_t, list_W_previous,\n",
    "                    list_pf_value_previous, list_dailyReturn_t)\n",
    "    eval_perf(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T03:24:22.473379Z",
     "start_time": "2018-07-08T03:22:45.265967Z"
    },
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PZpZCKcwCCK4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#######TEST#######\n",
    "\n",
    "\n",
    "#initialization of the environment \n",
    "state, done = env.reset(w_init_test, pf_init_test, t = total_steps_train)\n",
    "\n",
    "state_eq, done_eq = env_eq.reset(w_eq, pf_init_test, t = total_steps_train)\n",
    "state_s, done_s = env_s.reset(w_s, pf_init_test, t = total_steps_train)\n",
    "\n",
    "for i in range(m):\n",
    "    state_fu[i],  done_fu[i] = env_fu[i].reset(action_fu[i], pf_init_test, t = total_steps_train)\n",
    "\n",
    "\n",
    "#first element of the weight and portfolio value \n",
    "p_list = [pf_init_test]\n",
    "w_list = [w_init_test]\n",
    "\n",
    "p_list_eq = [pf_init_test]\n",
    "p_list_s = [pf_init_test]\n",
    "\n",
    "\n",
    "p_list_fu = list()\n",
    "for i in range(m):\n",
    "    p_list_fu.append([pf_init_test])\n",
    "    \n",
    "pf_value_t_fu = [0]*m\n",
    "    \n",
    "\n",
    "for k in range(total_steps_train +total_steps_val-int(n/2), total_steps_train +total_steps_val +total_steps_test -n):\n",
    "    X_t = state[0].reshape([-1]+ list(state[0].shape))\n",
    "    W_previous = state[1].reshape([-1]+ list(state[1].shape))\n",
    "    pf_value_previous = state[2]\n",
    "    #compute the action \n",
    "    action = actor.compute_W(X_t, W_previous)\n",
    "    #step forward environment \n",
    "    state, reward, done = env.step(action)\n",
    "    state_eq, reward_eq, done_eq = env_eq.step(w_eq)\n",
    "    state_s, reward_s, done_s = env_s.step(w_s)\n",
    "    \n",
    "    \n",
    "    for i in range(m):\n",
    "        state_fu[i], _ , done_fu[i] = env_fu[i].step(action_fu[i])\n",
    "    \n",
    "    \n",
    "    X_next = state[0]\n",
    "    W_t = state[1]\n",
    "    pf_value_t = state[2]\n",
    "    \n",
    "    pf_value_t_eq = state_eq[2]\n",
    "    pf_value_t_s = state_s[2]\n",
    "    for i in range(m):\n",
    "        pf_value_t_fu[i] = state_fu[i][2]\n",
    "    \n",
    "    dailyReturn_t = X_next[-1, :, -1]\n",
    "    if k%20 == 0:\n",
    "        print('current portfolio value', round(pf_value_previous,0))\n",
    "        print('weights', W_previous)\n",
    "    p_list.append(pf_value_t)\n",
    "    w_list.append(W_t)\n",
    "    \n",
    "    p_list_eq.append(pf_value_t_eq)\n",
    "    p_list_s.append(pf_value_t_s)\n",
    "    for i in range(m):\n",
    "        p_list_fu[i].append(pf_value_t_fu[i])\n",
    "        \n",
    "    #here to breack the loop/not in original code     \n",
    "    if k== total_steps_train +total_steps_val-int(n/2) + 100:\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T14:16:10.728927Z",
     "start_time": "2018-07-08T14:16:10.671401Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"individual_stocks_5yr/\"\n",
    "times = pd.read_csv(path+\"A_data.csv\").date\n",
    "test_start_day =total_steps_train +total_steps_val-int(n/2)+10\n",
    "times = list(times[test_start_day:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T14:16:12.074157Z",
     "start_time": "2018-07-08T14:16:11.667043Z"
    }
   },
   "outputs": [],
   "source": [
    "#batch_size, learning, ratio_greedy, e, n, kernel1_size, n_batches, ratio_regul\n",
    "\n",
    "data_type = path_data.split('/')[2][5:].split('.')[0]\n",
    "namesBio=['JNJ','PFE','AMGN','MDT','CELG','LLY']\n",
    "namesUtilities=['XOM','CVX','MRK','SLB','MMM']\n",
    "namesTech=['FB','AMZN','MSFT','AAPL','T','VZ','CMCSA','IBM','CRM','INTC']\n",
    "\n",
    "\n",
    "if data_type == 'Utilities':\n",
    "    list_stock = namesUtilities\n",
    "elif data_type == 'Bio':\n",
    "    list_stock = namesBio\n",
    "elif data_type == 'Tech':\n",
    "    list_stock = namesTech\n",
    "else:\n",
    "    list_stock = [i for i in range(m)]\n",
    "\n",
    "\n",
    "plt.title('Portfolio Value (Test Set) {}: {}, {}, {}, {}, {}, {}, {}, {}'.format(data_type, batch_size, learning, ratio_greedy, e, n, kernel1_size, n_batches, ratio_regul))\n",
    "plt.plot(p_list, label = 'Agent Portfolio Value')\n",
    "plt.plot(p_list_eq, label = 'Equi-weighted Portfolio Value')\n",
    "plt.plot(p_list_s, label = 'Secured Portfolio Value')\n",
    "for i in range(m):\n",
    "    plt.plot(p_list_fu[i], label = 'Full Stock {} Portfolio Value'.format(list_stock[i]))\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T14:16:13.859470Z",
     "start_time": "2018-07-08T14:16:13.563458Z"
    }
   },
   "outputs": [],
   "source": [
    "names = ['Money'] + list_stock\n",
    "w_list = np.array(w_list)\n",
    "for j in range(m+1):\n",
    "    plt.plot(w_list[:,j], label = 'Weight Stock {}'.format(names[j]))\n",
    "    plt.title('Weight evolution during testing')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T14:16:19.492492Z",
     "start_time": "2018-07-08T14:16:19.284505Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.array(p_list)-np.array(p_list_eq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T14:16:22.195478Z",
     "start_time": "2018-07-08T14:16:22.013562Z"
    }
   },
   "outputs": [],
   "source": [
    "index1=0\n",
    "index2=-1\n",
    "\n",
    "plt.plot(list_final_pf[index1:index2], label = 'Agent Portfolio Value')\n",
    "plt.plot(list_final_pf_eq[index1:index2], label = 'Baseline Portfolio Value')\n",
    "plt.plot(list_final_pf_s[index1:index2], label = 'Secured Portfolio Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T14:16:23.038728Z",
     "start_time": "2018-07-08T14:16:22.867818Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot((np.array(list_final_pf)-np.array(list_final_pf_eq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T18:54:39.717210Z",
     "start_time": "2018-05-05T18:54:39.712361Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "DPM_v2.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "219px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "696px",
    "left": "611.992px",
    "right": "20px",
    "top": "113.984px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
